{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abdf785",
   "metadata": {},
   "source": [
    "#Ans1.) \n",
    "\n",
    "Simple Linear Regression: Simple linear regression involves predicting a dependent variable (target) based on a single independent variable (feature). \n",
    "\n",
    "Multiple Linear Regression: Multiple linear regression extends simple linear regression to predict a dependent variable using multiple independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ebecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45bc334f",
   "metadata": {},
   "source": [
    "#Ans2.) \n",
    "\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables should be linear.\n",
    "Independence: The observations should be independent of each other.\n",
    "Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\n",
    "Normality of Residuals: The residuals should be normally distributed.\n",
    "To check these assumptions, diagnostic plots such as residual plots, normal probability plots, and scatterplots can be used. Statistical tests like the Durbin-Watson test for autocorrelation and the Breusch-Pagan test for homoscedasticity can also be employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12355cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea327b6c",
   "metadata": {},
   "source": [
    "#Ans3.) \n",
    "\n",
    "Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope: It represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant. For example, if the slope coefficient for the area of a house is 50, it means that for every additional square meter of area, the house price increases by 50 units.\n",
    "\n",
    "Intercept: It represents the value of the dependent variable when all independent variables are zero. In practical terms, it may or may not have a meaningful interpretation depending on the context. For example, in the house price prediction scenario, the intercept could represent the base price of a house with zero area, which might not be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12226c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa3a6a5",
   "metadata": {},
   "source": [
    "#Ans4.) \n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function (cost function) in machine learning models, including linear regression. It works by iteratively updating the parameters (coefficients) of the model in the opposite direction of the gradient of the loss function with respect to the parameters. This process continues until convergence, where the gradient approaches zero.\n",
    "\n",
    "In linear regression, gradient descent is used to find the optimal values of the coefficients (slope and intercept) that minimize the difference between the predicted values and the actual values of the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73505950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb9422cc",
   "metadata": {},
   "source": [
    "#Ans5.) \n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that involves predicting a dependent variable using multiple independent variables. It allows for a more complex modeling of the relationship between the dependent variable and the predictors by considering the combined effect of multiple variables simultaneously.\n",
    "\n",
    "Unlike simple linear regression, which has only one independent variable, multiple linear regression can accommodate multiple independent variables in the model equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e14813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e652fc0",
   "metadata": {},
   "source": [
    "#Ans6.) \n",
    "\n",
    "Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. It can cause issues such as inflated standard errors of coefficients and difficulties in interpreting the individual effects of predictors.\n",
    "\n",
    "To detect multicollinearity, techniques such as correlation matrices, variance inflation factors (VIF), and eigenvalue decomposition can be used. Addressing multicollinearity may involve removing one of the correlated variables, transforming variables, or using regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3469d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c77c92",
   "metadata": {},
   "source": [
    "#Ans7.) \n",
    "\n",
    "Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial. It allows for a nonlinear relationship between the variables.\n",
    "\n",
    "Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture more complex relationships that cannot be adequately modeled by a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697d288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9adf1f0",
   "metadata": {},
   "source": [
    "#Ans8.) \n",
    "\n",
    "Advantages and Disadvantages of Polynomial Regression:\n",
    "\n",
    "Advantages:\n",
    "Flexibility: It can capture nonlinear relationships between variables.\n",
    "Simple Implementation: It can be implemented using standard regression techniques.\n",
    "Disadvantages:\n",
    "Overfitting: Higher-degree polynomials can lead to overfitting, especially with limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed731ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4e94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
